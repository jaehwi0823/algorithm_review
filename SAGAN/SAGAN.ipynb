{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Generative Adversarial Networks (2018.05.)\n",
    "\n",
    "\n",
    "<br> 2018/09/01\n",
    "<br> Jaehwi Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "최근까지의 GAN모델은 texture patterns는 잘 잡아내는 반면, geometric or structural patterns을 잘 잡아내지 못한다는 문제점이 있다고 합니다.\n",
    "이에 대한 가장 큰 이유로, Conv Layer가 Localized된 정보를 주로 전달하기 때문이라고 합니다. 이를 해결하기 위해서 Receptive Field 또는 Stride를 키울 수 있지만, 이에 따른 부작용도 함께 발생합니다. 그래서 __Self-Attention__ 방법을 사용합니다.\n",
    "\n",
    "![Figure1](./Figure1.png)\n",
    "\n",
    "본 논문에서는 Self-attention + GAN (SAGAN)을 제안합니다. Self-attention module 덕분에 long range & multi-level dependencies across\n",
    "image region을 고려할 수 있게 됐습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "\n",
    "### 2.1 GAN\n",
    "\n",
    "    - skip\n",
    "\n",
    "### 2.2 Attention Models\n",
    "\n",
    "    - 더 자세하게 만들 자신이 없어서 링크 겁니다...\n",
    "\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention Generative Adversarial Networks\n",
    "\n",
    "\"GAN + Attention\"이 이 논문의 Main Contribution 입니다.\n",
    "\n",
    "![Figure2](Figure2.png)\n",
    "\n",
    "> 1. 일단 Conv를 통과시킨 Feature Map을 Channel X (W*H) 차원으로 바꿔줍니다 $ x \\in \\mathbb{R}^{C X N} $\n",
    "> 2. Attention Matrix를 각각 준비합니다. (QUERY~F(x), KEY~G(x), VALUE~H(x))\n",
    "> 3. Attention output을 계싼합니다. \n",
    "    - $o_j = (F^T \\otimes G) \\odot H$ \n",
    "> 4. 쉬운 task로 먼저 학습시키고, 점진적으로 어려운 task를 학습시킬 수 있도록 합니다.\n",
    "    - $y_i = \\gamma o_i + x_i$\n",
    "    - where $\\gamma$ is innitilized as 0\n",
    "> 5. Adversarial Loss는 Hinge Loss version을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Techniques to stabilize GAN training\n",
    "### 4.1 Spectral Normalization for both generator and discriminator\n",
    "\n",
    "> 1. lipschitz를 반영하는 GAN모델들이 다음과 같음\n",
    "    - GAN -> WGAN -> WGAN-GP -> Spectral Normalization(SN)\n",
    "> 2. SN논문(2018.02)에서는 Discriminator에만 반영하는 것을 제안했는데, Generator에도 SN을 쓰면 좋을 것 같아서 썼다고 합니다.\n",
    "\n",
    "### 4.2 Imbalanced learning rate for generator and discriminator updates\n",
    "\n",
    "> SN논문에서 Regularized Discriminator를 계산할 때, 학습이 둔화되는 문제가 있었습니다. <br>\n",
    "> 그래서 TTUR이라는 기법을 사용하여, [Discriminator Steps >= Generator Steps] 하도록 하여 Discriminator의 학습 속도를 향상시킵니다. <br>\n",
    "> 같은 wall-clock time에서 더 좋은 결과를 만들어 낼 수 있었다고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc58cae85133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## [[ Implementation ~ https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSelf_Attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\" Self attention Layer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "## [[ Implementation ~ https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py]]\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim,activation):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
