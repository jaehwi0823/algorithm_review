{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of Single-Layer Networks in Unsupervised Feature Learning\n",
    "\n",
    "2018.6.9 <br>\n",
    "Jaehwi Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "<br>\n",
    "본 논문에서는 몇몇 매우매우 간단한 요소들이 매우 중요하다는 것을 보였다고 합니다. <br>\n",
    "아래는 Set-up에 대한 요약입니다.\n",
    "\n",
    "1. Benchmark Datasets\n",
    "> - NORB: intended for 3d object detection, last update:2005  <br>\n",
    "> - CIFAR\n",
    "\n",
    "2. Off-the-shelf feature learning algorithms\n",
    "> - sparse auto-encoders\n",
    "> - sparse RBMs\n",
    "> - K-means clustering\n",
    "> - Gaussian mixtures\n",
    "\n",
    "3. Network: only single-layer networks\n",
    "\n",
    "4. Details to vary\n",
    "> - the receptive field size\n",
    "> - number of hidden nodes\n",
    "> - the step-size (stride)\n",
    "> - whitening\n",
    "\n",
    "\n",
    "본 논문에서 알고리즘의 선택만큼이나 매우 중요하다고 확인된 두 가지는 아래와 같습니다. \n",
    "> - the large numbers of hidden nodes\n",
    "> - Dense feature extraction\n",
    "\n",
    "이 두 가지를 pushed to limits 했을 때, CIFAR & NORB 모두에서 SOTA 결과를 얻었다고 합니다. <br>\n",
    "더 놀라운 사실은 __K-means clustering 에서 가장 좋은 결과__를 얻었다는 것 입니다. 매우 빠르고, 모형 구조 설계 외의 Hyper-parameters도 없는 알고리즘인데도 SOTA 결과를 보여줬다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "일단 옛날 논문이라 좋은 feature을 만들어내는 Current solution을 이렇게 표현합니다.\n",
    "> greedily \"pre-training\" several layers of features, one layer at a time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related work\n",
    "\n",
    "Deep한 구조를 위한 many new schemes가 제안됐으나 비지도 학습 모듈이 가장 심도있게 연구됐다고 합니다.\n",
    "\n",
    "이전 연구로는 ..\n",
    "- pooling, normalization, rectification between layers\n",
    "- pooling activation function or coding scheme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unsupervised feature learning framework\n",
    "\n",
    "### 3.1 Feature Learning\n",
    "\n",
    "Extracting random sub-patches from unlabeled input images\n",
    ">  - Each patch has dimension [w, w, d]\n",
    ">  - then, construct a dataset of m randomly sampled patches\n",
    ">  - apply the pre-processing and unsupervised learning steps\n",
    "\n",
    "<br>\n",
    "**3.1.1. Pre-processing**\n",
    "> - Normalization\n",
    "> - Whitening는 실험대상\n",
    "\n",
    "**3.1.2. Unsupervised learning**\n",
    "> 1. Sparse auto-encoders\n",
    "> 2. Sparse RBMs\n",
    "> 3. K-means clustering\n",
    "> 4. Gaussian mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "** Sparse auto-encoder **\n",
    "\n",
    "[Autoencoder](http://solarisailab.com/archives/113)\n",
    "> - 항등함수를 학습하여 함축적인 Featuer 탐색\n",
    "> - Sparsity 부여\n",
    "\n",
    "\n",
    "![Autoencoder](autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "__Sparse restricted Boltzmann machine (RBM)__\n",
    "\n",
    "모르겠어요...<br>\n",
    "help me!\n",
    "\n",
    "[보충설명](http://sanghyukchun.github.io/75/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__K-means clustering__\n",
    "\n",
    "[K-menas](http://sanghyukchun.github.io/69/)\n",
    "> - 간단함\n",
    "> - 그 대신 Local Minima에 수렴\n",
    "\n",
    "![K-means](./K-means.png)\n",
    "\n",
    "<br>\n",
    "논문에서는 두 가지 K-means를 사용함. $f_k(x)$가 input x가 k번째 cluster에 속하는 정도일때,\n",
    "> - the standard 1-of-K: (hard)\n",
    ">> $f_k(x)=1$ if $k= argmin_j||c^{(j)} - x||_2^2$ <br> otherwise 0\n",
    "> - softer version: (triangle)\n",
    ">>  $f_k(x) = max(0, \\mu (z) - z_k)$ <br> $z_k = ||x-c^{(k)}||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__Gaussian mixtueres (GMM)__\n",
    "\n",
    "[GMM](http://sanghyukchun.github.io/69/)\n",
    "> - EM 알고리즘으로 솔루션 탐색\n",
    "> - 초기값은 K-means 알고리즘 활용\n",
    "\n",
    "![GMM](http://sanghyukchun.github.io/images/post/69-4.gif)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Featuer Extraction and Classification\n",
    "\n",
    "위 알고리즘으로 추출된 \"new representation\"을 labeled training images에 적용해서 분류를 학습합니다.\n",
    "\n",
    "__3.2.1 Convolutional Extraction__\n",
    "> - 한 개의 patch의 \"new representation\"을 얻기 위해 특징추출기 f를 many sub-patches에 적용합니다.\n",
    "> - [n, n, d] -> [(n-w)/s+1, (n-w)/s+1, k]\n",
    "> - pooling over 4 quadrants -> summation\n",
    "> - create a feature vector for classification\n",
    "\n",
    "![Figure1](./Figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.2.2 Classification__\n",
    "\n",
    "SVM with L2를 사용했고, hyper parameters는 cross-validation으로 결정했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments and Analysis\n",
    "\n",
    "실험 옵션들은 아래와 같습니다.\n",
    "> 1. Whitenming\n",
    "> 2. the number of features K\n",
    "> 3. the stride s\n",
    "> 4. receptive field size w\n",
    "\n",
    "![Figure3](./Figure3.png)\n",
    "![Figure4](./Figure4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualization\n",
    "\n",
    "> - Autoencoders, sparse RBMs가 Gabor filters와 비슷한 결과를 본 논문 전에도 이미 잘 알려져 있었답니다.\n",
    "> - Clustering Algorithms도 비슷한 결과를 처음이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Effect of whitening\n",
    "> - autoencoders and RBM에서는 역할이 애매하다. <br>\n",
    "(feature 수를 어짜피 많이 가져갈 거니까)\n",
    "> - Clustering Algorithm에서는 whitening 차이가 크다. <br>\n",
    "(clustering에서는 데이터의 correlations를 고려하지 않기 때문이라는데)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Number of features\n",
    "\n",
    "> - Feature 갯수를 바꿔가며 실험: 100 / 200 / 400 / 800 / 1200 / 1600\n",
    "> - Feature 수가 많으면 많을 수록 좋은 성능을 보임\n",
    "> - Hyper parameter tuning이 필요없는 __K-means clustering의 성능이 제일 좋았다는 것__이 주목할 만한 성과라 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Effect of stride\n",
    "\n",
    "> - stride 크기에 따른 연산비용이 부담되지만.. Stride는 1일때가 성능이 제일 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Effect of receptive field size\n",
    "\n",
    "> - small size works better\n",
    "> - especially, when the input size is small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
