{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "\n",
    "2018.11.04. <br>\n",
    "Jaehwi Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "> - GBM의 성능이 뛰어나지만, 데이터의 크기가 커지며 Accuracy <-> Efficiency Trade-off 문제가 발생하기 시작함\n",
    "> - 기존 GBM은 매번 모든 Data를 탐색해 Information Gain을 구해야 했음... 계산복잡도에 Feature 수뿐만 아니라 Data 수도 같이 고려됨\n",
    "> - 본 논문에서는 두 가지 Novel Points를 제시함\n",
    ">> GOSS(Gradient-Based One-Side Sampling)\n",
    ">> - 더 큰 Gradient를 가진 Data Instances가 Information Gain에 더 많이 기여함\n",
    ">> - 이러한 방식으로 랜덤 샘플링보다는 정교한 I.G. 예측이 가능해졌음\n",
    ">>\n",
    ">> EFB(Exclusive Feature Bundling)\n",
    ">> - 실제 Application에서, Features의 숫자는 많지만 feature space는 상당히 sparse함\n",
    ">> - 대부분의 그런 데이터는 제외할만 함\n",
    ">> - 그리고 one-hot encoding된 변수들도 대부분 그러함\n",
    ">> - 본 논문에서 안전하게 그러한 exclusive features를 한 데 묶는 방법을 제시함\n",
    ">> - Bundling Problem을 Graph Coloring Problem으로 치환하며, Greedy Algorithm으로 해를 구함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preliminaries\n",
    "### 2.1 GBDT and Its Complexity Analysis\n",
    "\n",
    "> - GBDT는 DT의 앙상블 모델이며, 차례차례 학습이 진행됨\n",
    "> - 매 iteration에서 negative gradient(residual errors)를 적합하며 학습이 진행됨\n",
    "\n",
    "---\n",
    "> - DT 학습에서 가장 큰 시간 소모는 split-point를 찾는 것임\n",
    "> - DT의 학습속도를 향상시키기위해 the pre-sorted algorithm 대신 the histogram-based algorithm이 사용됨\n",
    "> - Histogram 생성에 #data$\\times$#features, Split point 탐색에 #bin$\\times$#features 이므로 최종 복잡도는 #data, #features에 달려있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 Related work\n",
    "> - GBDT의 implementation은 여러가지가 있지만 성능이 가장 좋은 XGBoost를 Base model로 사용함\n",
    "\n",
    "---\n",
    "> - 데이터 샘플링이 속도를 증가시키기 위한 일반적인 방법이며, SGB 알고리즘은 특정 가중치 이하의 sample을 모두 filtering 함\n",
    "> - 그러나 모든 데이터가 각각의 가중치를 갖고 있지 않으므로 GBDT에서 사용할 수는 없음\n",
    "> - 주성분분석이나 사영추적 방법을 사용할 수도 있지만, 사라지는 변수의 영향도를 무시할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient-based One-Side Sampling (GOSS)\n",
    "### 3.1 Algorithm Description\n",
    "\n",
    "> - small-gradient의 의미는 에러가 작다는 의미이므로 해당 instance가 well-trained 됐다고 생각할 수 있음\n",
    "> - 그러므로 gradient의 값이 큰 것들만 선택하면 되지만, 그러면 샘플 분포가 틀어질 수 있으므로 GOSS 방법을 제안함\n",
    "\n",
    "![g](./gradient.png)\n",
    "\n",
    "출처: http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "\n",
    "---\n",
    "> - GOSS에서 gradient 값이 큰 instances는 모두 보관하고, gradient 값이 작은 instances에 대해서는 Random Sampling을 진행함\n",
    "> - gradient 순으로 정렬한 상태에서 값이 큰 상위 a% instance는 모두 선택하고, 나머지 데이터 중 b%의 instances를 Random Sampling 함\n",
    "> - 분포가 틀어지는 것을 막기 위해 Random Sample의 Information Gain 계산시, 상수 \"(1-a)/b \"를 곱해줌\n",
    "\n",
    "### 3.2 Theoretical Analysis\n",
    "\n",
    ">__definition 3.1__\n",
    ">\n",
    "> - sampling 된 데이터만으로 I.G를 계싼함\n",
    "> - I.G. 계산할 때 low gradient 그룹은 (1-a)/b 값을 곱해줌\n",
    "> - 이렇게 함으로써, under-trained data에 대해 더 집중할 수 있음\n",
    "> - 그리고 샘플링 된 데이터로 계산하므로 연산 속도도 빨라지는 장점이 있음\n",
    ">\n",
    "> ![3_1](./definition3_1.png)\n",
    "\n",
    "\n",
    "---\n",
    "> __Theorem 3.2__\n",
    ">\n",
    "> hmmm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exclusive Feature Bundling\n",
    "\n",
    "> - Sparse한 데이터가 너무 많으므로 잘 묶어줘야 함\n",
    "> - 1) 어떤 변수들을 묶어줘야 하는지, 2) 묶는 방법이 두 가지 메인 이슈임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 4.1 Bundling 문제는 Graph-Coloring 문제로 치환할 수 있음.. 그렇다고 함...\n",
    "\n",
    "![CG](./graph_coloring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 모든 features를 Graph의 Vertex로 생각하고, 서로 Mutually Exclusive가 아닐 경우에만 Edge를 연결\n",
    "> - 그 상태에서 Greedy하게 문제를 해결하면 좋은 근사해가 됨\n",
    "> - 또한, 100% M.E.는 아니지만 꽤나 많은 변수가 0이 아닌 값을 동시에 갖지 않음\n",
    "> - 우리가 그러한 충돌을 조금 인정한다면, 변수의 수가 더 크게 줄어듦\n",
    "> - 알고리즘 스텝은,\n",
    ">> - 1) 모든 features간 edge를 연결하며 edge의 weight로 변수간 conflicts 값을 갖게 함\n",
    ">> - 2) features를 다른 features들과 연결된 숫자인 degree 순으로 정렬함\n",
    ">> - 3) 작은 conflict 값은 무시하며 greedy algorithm으로 기존 번들에 넣거나 새로운 번들을 만들어 냄\n",
    "> - 본 작업은 전체 lightGBM 로직에서 가장 처음에 한 번만 하면 되고,\n",
    "> - 연산복잡도는 (#features)^2 임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> 같은 번들 내 변수를 묶는 방법은,\n",
    "> - offset 을 설정하여 값의 범위가 겹치지 않게 만들고\n",
    "> - 원래 변수들을 새로운 값으로 대체함\n",
    "> ![algorithm4](./algorithm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Data pipeline\n",
    "\n",
    "# 1.1 lgb를 위한 데이터 생성.. \n",
    "# libsvm text file(.svm) 또는 lightGBM binary(.bin)이 기본\n",
    "# 그런데 그냥 txt도 가능\n",
    "train_data = lgb.Dataset('train.txt')\n",
    "\n",
    "# 1.2 lightGBM binary file로 저장해두면 속도가 빠르다고 함\n",
    "train_data.save_binary('train.bin')\n",
    "\n",
    "# 1.3 Validation set \n",
    "# 생성은\n",
    "test_data = train_data.create_valid('test.svm')\n",
    "test_data = lgb.Dataset('test.svm', reference=train_data)\n",
    "# val set은 항상 준비돼있어야 한다고 함\n",
    "\n",
    "# 1.4 Data feature 지정\n",
    "# lightGBM에서 categorical은 그냥 써주라고 함. one-hot encoding이 필요 없고, 원상태로 두는게 8배정도 더 빠르다고 함\n",
    "# 다만, categorical 데이터를 int 형식으로 미리 만들어야 함. 문자 안돼요!\n",
    "train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Parameter Setting\n",
    "\n",
    "# 2.1 단일 지정\n",
    "param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "# 2.2 복수 지정\n",
    "param['metric'] = ['auc', 'binary_logloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Training\n",
    "\n",
    "# 3.1 학습\n",
    "bst = lgb.train(param, train_data, num_round=10, valid_sets=[test_data])\n",
    "\n",
    "# 3.2 저장\n",
    "bst.save_model('model.txt')\n",
    "json_model = bst.dump_model()\n",
    "\n",
    "# 3.3 불러오기\n",
    "bst = lgb.Booster(model_file='model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Inference\n",
    "ypred = bst.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
